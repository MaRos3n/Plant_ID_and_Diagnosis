{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd72216-59ac-4b98-9eaf-e32e3a38d434",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdba39-cbf6-43f4-9ae1-0285caac4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bab3c9-f07a-494b-ae94-3c217d8f8777",
   "metadata": {},
   "source": [
    "# Train/Validate/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df9349-1938-460b-a9a1-7970ceb0cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "merged_dataset_path = '/Users/maggie/Desktop/project_3/Plant_ID_and_Diagnosis/Resources/plant_dr_master_dataset/master_dataset'\n",
    "combined_csv_path = '/Users/maggie/Desktop/project_3/Plant_ID_and_Diagnosis/Resources/plant_dr_master_dataset/master_dataset/combined_labels.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df.head())\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "# Print the distribution to verify the split\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(valid_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# ImageDataGenerator setup\n",
    "img_size = (128, 128)\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,       # Randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,   # Randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.2,         # Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n",
    "    zoom_range=0.2,          # Randomly zoom image\n",
    "    horizontal_flip=True,    # Randomly flip images horizontally\n",
    "    fill_mode='nearest'      # Points outside the boundaries of the input are filled according to the given mode\n",
    ")\n",
    "\n",
    "# Load training data using flow_from_dataframe\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',  # Column name in the CSV file that contains the image file names\n",
    "    y_col='label',  # Column name in the CSV file that contains the labels\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load validation data using flow_from_dataframe\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=valid_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load test data using flow_from_dataframe\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # No need to shuffle for testing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df2db4-9067-49bd-969b-eeeeded466c9",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4c523-88b7-4209-b880-f393821b4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Learning - Apply only after training is complete\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Extract features\n",
    "feature_model = Model(inputs=model.input, outputs=model.layers[-2].output)  # Using the layer before the output\n",
    "features = feature_model.predict(train_generator)  # Using the training set for feature extraction\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=30)\n",
    "reduced_features = pca.fit_transform(features)\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('K-Means Clustering of Plant and Ailment Features')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43152d1f-8f60-41b0-9cb6-8436b33a8e75",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ecbb3-6f14-4532-a5d7-259f4a40edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x1 = Flatten()(base_model.output)\n",
    "x2 = Dense(1024, activation='relu')(x1)\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(train_generator.class_indices)  # Number of unique classes\n",
    "\n",
    "# Output layer\n",
    "x3 = Dense(num_classes, activation='softmax')(x2)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b1c4e0-2bfb-45e4-9af5-6f2981286f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=valid_generator, epochs=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609b2e8-ab91-4683-928b-8a060b34a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(valid_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728628ea-442b-4351-a6cd-26806879ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last few layers of the VGG16 model for fine-tuning\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Re-train the model with fine-tuning\n",
    "fine_tune_history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d076723-3923-4e6b-a360-61217c74788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model on the test set\n",
    "fine_tuned_test_loss, fine_tuned_test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Fine-Tuned Test Loss: {fine_tuned_test_loss}\")\n",
    "print(f\"Fine-Tuned Test Accuracy: {fine_tuned_test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a36df5-25fe-4cea-b6a3-322ce6d8198c",
   "metadata": {},
   "source": [
    "# Misclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a72b51-c4f4-4fbb-8b64-0cc72c593af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictions for the test set\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Identify misclassified indices\n",
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "# Map class indices to class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Display some misclassified images\n",
    "num_misclassified_to_display = 10  # Number of misclassified images to display\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, index in enumerate(misclassified_indices[:num_misclassified_to_display]):\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img_path = test_generator.filepaths[index]\n",
    "    img = keras_image.load_img(img_path, target_size=(128, 128))\n",
    "    img = keras_image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0  # Rescale if needed\n",
    "    \n",
    "    plt.imshow(img[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    true_label = class_labels[true_labels[index]]\n",
    "    predicted_label = class_labels[predicted_labels[index]]\n",
    "    plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd4b43-a16e-43c7-9763-ce6442694c13",
   "metadata": {},
   "source": [
    "# Predict on Unseen Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a7039-dfca-4a15-8695-0679c3715f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=img_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Rescale the image\n",
    "    return img_array\n",
    "\n",
    "# Predict a single image\n",
    "new_image_path = '/Users/maggie/Desktop/project_3/Plant_ID_and_Diagnosis/Resources/plant_identifcation_resources/crop_pest_and_disease/tomato/healthy/healthy16_.jpg'  # Replace with your image path\n",
    "img_array = load_and_preprocess_image(new_image_path)\n",
    "prediction = model.predict(img_array)\n",
    "\n",
    "# Map the prediction to class names\n",
    "predicted_class_index = np.argmax(prediction)\n",
    "predicted_class_name = list(train_generator.class_indices.keys())[predicted_class_index]\n",
    "\n",
    "print(f\"Predicted class: {predicted_class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1633a-a880-41f9-8820-05354eb2c3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

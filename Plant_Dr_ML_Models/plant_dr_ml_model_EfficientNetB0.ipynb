{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e1d33e-970a-43fe-9e6c-b24bb3414e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6df9349-1938-460b-a9a1-7970ceb0cef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 56641\n",
      "Validation set size: 12137\n",
      "Test set size: 12138\n",
      "Found 56641 validated image filenames belonging to 171 classes.\n",
      "Found 12137 validated image filenames belonging to 171 classes.\n",
      "Found 12138 validated image filenames belonging to 171 classes.\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 469ms/step - accuracy: 0.0634 - loss: 4.2645 - val_accuracy: 0.0662 - val_loss: 4.2227\n",
      "Epoch 2/7\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m845s\u001b[0m 477ms/step - accuracy: 0.0668 - loss: 4.2099 - val_accuracy: 0.0681 - val_loss: 4.2121\n",
      "Epoch 3/7\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 491ms/step - accuracy: 0.0676 - loss: 4.2040 - val_accuracy: 0.0662 - val_loss: 4.2116\n",
      "Epoch 4/7\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 488ms/step - accuracy: 0.0688 - loss: 4.2094 - val_accuracy: 0.0629 - val_loss: 4.2099\n",
      "Epoch 5/7\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m931s\u001b[0m 525ms/step - accuracy: 0.0675 - loss: 4.2044 - val_accuracy: 0.0662 - val_loss: 4.2034\n",
      "Epoch 6/7\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m947s\u001b[0m 534ms/step - accuracy: 0.0664 - loss: 4.2071 - val_accuracy: 0.0662 - val_loss: 4.2107\n",
      "Epoch 7/7\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m892s\u001b[0m 503ms/step - accuracy: 0.0648 - loss: 4.2075 - val_accuracy: 0.0681 - val_loss: 4.2041\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 426ms/step - accuracy: 0.0668 - loss: 4.1801\n",
      "Test Accuracy: 6.81%\n"
     ]
    }
   ],
   "source": [
    "# Supervised Learning\n",
    "\n",
    "# Paths\n",
    "merged_dataset_path = '/Users/maggie/Desktop/project_3/Plant_ID_and_Diagnosis/Resources/plant_dr_master_dataset/master_dataset'\n",
    "combined_csv_path = '/Users/maggie/Desktop/project_3/Plant_ID_and_Diagnosis/Resources/plant_dr_master_dataset/master_dataset/combined_labels.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "# Print the distribution to verify the split\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(valid_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# ImageDataGenerator setup\n",
    "img_size = (128, 128)\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,       # Randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,   # Randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.2,         # Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n",
    "    zoom_range=0.2,          # Randomly zoom image\n",
    "    horizontal_flip=True,    # Randomly flip images horizontally\n",
    "    fill_mode='nearest'      # Points outside the boundaries of the input are filled according to the given mode\n",
    ")\n",
    "\n",
    "# Load training data using flow_from_dataframe\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',  # Column name in the CSV file that contains the image file names\n",
    "    y_col='label',  # Column name in the CSV file that contains the labels\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load validation data using flow_from_dataframe\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=valid_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load test data using flow_from_dataframe\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # No need to shuffle for testing\n",
    ")\n",
    "\n",
    "# Load the base model (EfficientNetB0)\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x1 = GlobalAveragePooling2D()(base_model.output)\n",
    "x2 = Dense(1024, activation='relu')(x1)\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(train_generator.class_indices)  # Number of unique classes\n",
    "\n",
    "# Output layer\n",
    "x3 = Dense(num_classes, activation='softmax')(x2)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x3)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=valid_generator, epochs=7)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d609b2e8-ab91-4683-928b-8a060b34a45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 420ms/step - accuracy: 0.0723 - loss: 4.2003\n",
      "Validation Loss: 4.204121112823486\n",
      "Validation Accuracy: 0.06805635988712311\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(valid_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b472f24-f938-4a2f-8817-04bf0ae5402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m973s\u001b[0m 542ms/step - accuracy: 0.0703 - loss: 4.2519 - val_accuracy: 0.0681 - val_loss: 4.2072\n",
      "Epoch 2/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 533ms/step - accuracy: 0.0668 - loss: 4.1949 - val_accuracy: 0.0681 - val_loss: 4.2060\n",
      "Epoch 3/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m949s\u001b[0m 535ms/step - accuracy: 0.0675 - loss: 4.2018 - val_accuracy: 0.0662 - val_loss: 4.1907\n",
      "Epoch 4/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m921s\u001b[0m 519ms/step - accuracy: 0.0699 - loss: 4.1843 - val_accuracy: 0.0739 - val_loss: 4.1785\n",
      "Epoch 5/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m942s\u001b[0m 531ms/step - accuracy: 0.0699 - loss: 4.1804 - val_accuracy: 0.0662 - val_loss: 4.1698\n",
      "Epoch 6/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 530ms/step - accuracy: 0.0700 - loss: 4.1699 - val_accuracy: 0.0662 - val_loss: 4.1600\n",
      "Epoch 7/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m951s\u001b[0m 536ms/step - accuracy: 0.0745 - loss: 4.1597 - val_accuracy: 0.0664 - val_loss: 4.1456\n",
      "Epoch 8/8\n",
      "\u001b[1m1771/1771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m920s\u001b[0m 519ms/step - accuracy: 0.0737 - loss: 4.1454 - val_accuracy: 0.0803 - val_loss: 4.1764\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the last few layers of the EfficientNetB0 model for fine-tuning\n",
    "for layer in base_model.layers[-20:]:  # Unfreeze the last 20 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Re-train the model with fine-tuning\n",
    "fine_tune_history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6b29f8-b2c4-45a1-ae44-91ef452df0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 379ms/step - accuracy: 0.0856 - loss: 4.1550\n",
      "Validation Loss: 4.176595687866211\n",
      "Validation Accuracy: 0.07901458442211151\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(valid_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b986ddd9-093b-41dd-80dd-6b4adc7672cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 132ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keras_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load and preprocess the image\u001b[39;00m\n\u001b[1;32m     19\u001b[0m img_path \u001b[38;5;241m=\u001b[39m test_generator\u001b[38;5;241m.\u001b[39mfilepaths[index]\n\u001b[0;32m---> 20\u001b[0m img \u001b[38;5;241m=\u001b[39m keras_image\u001b[38;5;241m.\u001b[39mload_img(img_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m     21\u001b[0m img \u001b[38;5;241m=\u001b[39m keras_image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[1;32m     22\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras_image' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAD4CAYAAAD1lwKKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZJklEQVR4nO3df0zV1/3H8deFKxd14zZqvYJSip2utKZ2XqIFR5p2eo0aO5cu0riIOk1K2s4fTFcpi1ZjQtqlZrUVbCtomqgj/ox/MOtNuin+2A8ZmqaQ2KgTbEECxgtqh4rn+4fj7nt3seXcci84no/k88d995z7ed+eIq+ez8fPdRhjjAAAANAjcX3dAAAAwIOE8AQAAGCB8AQAAGCB8AQAAGCB8AQAAGCB8AQAAGCB8AQAAGCB8AQAAGCB8AQAAGCB8AQAAGDBOjwdO3ZMc+bMUUpKihwOhw4ePPitc44ePSqv16vExESNHTtWW7dujaRXAACAPmcdnm7cuKGJEyfq/fff79H4ixcvatasWcrJyVFNTY3eeOMNLVu2TPv27bNuFgAAoK85vssXAzscDh04cEBz586975jXX39dhw4dUl1dXbCWn5+vs2fP6tSpU5GeGgAAoE84o32CU6dOyefzhdRmzJihsrIy3b59W4MGDQqb09HRoY6OjuDru3fv6urVqxo+fLgcDke0WwYAAP8DjDFqb29XSkqK4uJ67zbvqIenpqYmeTyekJrH49GdO3fU0tKi5OTksDnFxcVav359tFsDAAADQENDg8aMGdNr7xf18CQpbLeo60rh/XaRCgsLVVBQEHwdCAT0yCOPqKGhQUlJSdFrFAAA/M9oa2tTamqqvv/97/fq+0Y9PI0aNUpNTU0htebmZjmdTg0fPrzbOS6XSy6XK6yelJREeAIAAFZ6+5afqD/nKSsrS36/P6R25MgRZWZmdnu/EwAAQH9mHZ6uX7+uM2fO6MyZM5LuPYrgzJkzqq+vl3TvklteXl5wfH5+vi5duqSCggLV1dWpvLxcZWVlWrVqVe98AgAAgBiyvmx3+vRpPffcc8HXXfcmLVy4UDt27FBjY2MwSElSenq6KisrtXLlSm3ZskUpKSnavHmzXnzxxV5oHwAAILa+03OeYqWtrU1ut1uBQIB7ngAAQI9EKz/w3XYAAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWCE8AAAAWIgpPJSUlSk9PV2Jiorxer6qqqr5x/M6dOzVx4kQNGTJEycnJWrx4sVpbWyNqGAAAoC9Zh6eKigqtWLFCRUVFqqmpUU5OjmbOnKn6+vpuxx8/flx5eXlasmSJPv/8c+3Zs0d///vftXTp0u/cPAAAQKxZh6dNmzZpyZIlWrp0qTIyMvT73/9eqampKi0t7Xb8X/7yFz366KNatmyZ0tPT9eMf/1gvv/yyTp8+/Z2bBwAAiDWr8HTr1i1VV1fL5/OF1H0+n06ePNntnOzsbF2+fFmVlZUyxujKlSvau3evZs+efd/zdHR0qK2tLeQAAADoD6zCU0tLizo7O+XxeELqHo9HTU1N3c7Jzs7Wzp07lZubq4SEBI0aNUoPPfSQ3nvvvfuep7i4WG63O3ikpqbatAkAABA1Ed0w7nA4Ql4bY8JqXWpra7Vs2TKtXbtW1dXVOnz4sC5evKj8/Pz7vn9hYaECgUDwaGhoiKRNAACAXue0GTxixAjFx8eH7TI1NzeH7UZ1KS4u1tSpU7V69WpJ0lNPPaWhQ4cqJydHGzduVHJyctgcl8sll8tl0xoAAEBMWO08JSQkyOv1yu/3h9T9fr+ys7O7nXPz5k3FxYWeJj4+XtK9HSsAAIAHifVlu4KCAm3btk3l5eWqq6vTypUrVV9fH7wMV1hYqLy8vOD4OXPmaP/+/SotLdWFCxd04sQJLVu2TJMnT1ZKSkrvfRIAAIAYsLpsJ0m5ublqbW3Vhg0b1NjYqAkTJqiyslJpaWmSpMbGxpBnPi1atEjt7e16//339etf/1oPPfSQnn/+eb311lu99ykAAABixGEegGtnbW1tcrvdCgQCSkpK6ut2AADAAyBa+YHvtgMAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALBAeAIAALAQUXgqKSlRenq6EhMT5fV6VVVV9Y3jOzo6VFRUpLS0NLlcLj322GMqLy+PqGEAAIC+5LSdUFFRoRUrVqikpERTp07VBx98oJkzZ6q2tlaPPPJIt3PmzZunK1euqKysTD/4wQ/U3NysO3fufOfmAQAAYs1hjDE2E6ZMmaJJkyaptLQ0WMvIyNDcuXNVXFwcNv7w4cN66aWXdOHCBQ0bNiyiJtva2uR2uxUIBJSUlBTRewAAgIElWvnB6rLdrVu3VF1dLZ/PF1L3+Xw6efJkt3MOHTqkzMxMvf322xo9erTGjx+vVatW6euvv77veTo6OtTW1hZyAAAA9AdWl+1aWlrU2dkpj8cTUvd4PGpqaup2zoULF3T8+HElJibqwIEDamlp0SuvvKKrV6/e976n4uJirV+/3qY1AACAmIjohnGHwxHy2hgTVuty9+5dORwO7dy5U5MnT9asWbO0adMm7dix4767T4WFhQoEAsGjoaEhkjYBAAB6ndXO04gRIxQfHx+2y9Tc3By2G9UlOTlZo0ePltvtDtYyMjJkjNHly5c1bty4sDkul0sul8umNQAAgJiw2nlKSEiQ1+uV3+8Pqfv9fmVnZ3c7Z+rUqfrqq690/fr1YO3cuXOKi4vTmDFjImgZAACg71hftisoKNC2bdtUXl6uuro6rVy5UvX19crPz5d075JbXl5ecPz8+fM1fPhwLV68WLW1tTp27JhWr16tX/7ylxo8eHDvfRIAAIAYsH7OU25urlpbW7VhwwY1NjZqwoQJqqysVFpamiSpsbFR9fX1wfHf+9735Pf79atf/UqZmZkaPny45s2bp40bN/bepwAAAIgR6+c89QWe8wQAAGz1i+c8AQAADHSEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAuEJwAAAAsRhaeSkhKlp6crMTFRXq9XVVVVPZp34sQJOZ1OPf3005GcFgAAoM9Zh6eKigqtWLFCRUVFqqmpUU5OjmbOnKn6+vpvnBcIBJSXl6ef/OQnETcLAADQ1xzGGGMzYcqUKZo0aZJKS0uDtYyMDM2dO1fFxcX3nffSSy9p3Lhxio+P18GDB3XmzJken7OtrU1ut1uBQEBJSUk27QIAgAEqWvnBaufp1q1bqq6uls/nC6n7fD6dPHnyvvO2b9+u8+fPa926dZF1CQAA0E84bQa3tLSos7NTHo8npO7xeNTU1NTtnC+++EJr1qxRVVWVnM6ena6jo0MdHR3B121tbTZtAgAARE1EN4w7HI6Q18aYsJokdXZ2av78+Vq/fr3Gjx/f4/cvLi6W2+0OHqmpqZG0CQAA0OuswtOIESMUHx8ftsvU3NwcthslSe3t7Tp9+rRee+01OZ1OOZ1ObdiwQWfPnpXT6dSnn37a7XkKCwsVCASCR0NDg02bAAAAUWN12S4hIUFer1d+v18/+9nPgnW/36+f/vSnYeOTkpL02WefhdRKSkr06aefau/evUpPT+/2PC6XSy6Xy6Y1AACAmLAKT5JUUFCgBQsWKDMzU1lZWfrwww9VX1+v/Px8Sfd2jb788kt9/PHHiouL04QJE0Lmjxw5UomJiWF1AACAB4F1eMrNzVVra6s2bNigxsZGTZgwQZWVlUpLS5MkNTY2fusznwAAAB5U1s956gs85wkAANjqF895AgAAGOgITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYITwAAABYiCk8lJSVKT09XYmKivF6vqqqq7jt2//79mj59uh5++GElJSUpKytLn3zyScQNAwAA9CXr8FRRUaEVK1aoqKhINTU1ysnJ0cyZM1VfX9/t+GPHjmn69OmqrKxUdXW1nnvuOc2ZM0c1NTXfuXkAAIBYcxhjjM2EKVOmaNKkSSotLQ3WMjIyNHfuXBUXF/foPZ588knl5uZq7dq1PRrf1tYmt9utQCCgpKQkm3YBAMAAFa38YLXzdOvWLVVXV8vn84XUfT6fTp482aP3uHv3rtrb2zVs2LD7juno6FBbW1vIAQAA0B9YhaeWlhZ1dnbK4/GE1D0ej5qamnr0Hu+8845u3LihefPm3XdMcXGx3G538EhNTbVpEwAAIGoiumHc4XCEvDbGhNW6s3v3br355puqqKjQyJEj7zuusLBQgUAgeDQ0NETSJgAAQK9z2gweMWKE4uPjw3aZmpubw3aj/ltFRYWWLFmiPXv2aNq0ad841uVyyeVy2bQGAAAQE1Y7TwkJCfJ6vfL7/SF1v9+v7Ozs+87bvXu3Fi1apF27dmn27NmRdQoAANAPWO08SVJBQYEWLFigzMxMZWVl6cMPP1R9fb3y8/Ml3bvk9uWXX+rjjz+WdC845eXl6d1339UzzzwT3LUaPHiw3G53L34UAACA6LMOT7m5uWptbdWGDRvU2NioCRMmqLKyUmlpaZKkxsbGkGc+ffDBB7pz545effVVvfrqq8H6woULtWPHju/+CQAAAGLI+jlPfYHnPAEAAFv94jlPAAAAAx3hCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwALhCQAAwEJE4amkpETp6elKTEyU1+tVVVXVN44/evSovF6vEhMTNXbsWG3dujWiZgEAAPqadXiqqKjQihUrVFRUpJqaGuXk5GjmzJmqr6/vdvzFixc1a9Ys5eTkqKamRm+88YaWLVumffv2fefmAQAAYs1hjDE2E6ZMmaJJkyaptLQ0WMvIyNDcuXNVXFwcNv7111/XoUOHVFdXF6zl5+fr7NmzOnXqVI/O2dbWJrfbrUAgoKSkJJt2AQDAABWt/OC0GXzr1i1VV1drzZo1IXWfz6eTJ092O+fUqVPy+XwhtRkzZqisrEy3b9/WoEGDwuZ0dHSoo6Mj+DoQCEi69y8BAACgJ7pyg+U+0beyCk8tLS3q7OyUx+MJqXs8HjU1NXU7p6mpqdvxd+7cUUtLi5KTk8PmFBcXa/369WH11NRUm3YBAADU2toqt9vda+9nFZ66OByOkNfGmLDat43vrt6lsLBQBQUFwdfXrl1TWlqa6uvre/XDo3e1tbUpNTVVDQ0NXF7tp1ijBwPr9GBgnfq/QCCgRx55RMOGDevV97UKTyNGjFB8fHzYLlNzc3PY7lKXUaNGdTve6XRq+PDh3c5xuVxyuVxhdbfbzX+gD4CkpCTWqZ9jjR4MrNODgXXq/+LievfJTFbvlpCQIK/XK7/fH1L3+/3Kzs7udk5WVlbY+CNHjigzM7Pb+50AAAD6M+soVlBQoG3btqm8vFx1dXVauXKl6uvrlZ+fL+neJbe8vLzg+Pz8fF26dEkFBQWqq6tTeXm5ysrKtGrVqt77FAAAADFifc9Tbm6uWltbtWHDBjU2NmrChAmqrKxUWlqaJKmxsTHkmU/p6emqrKzUypUrtWXLFqWkpGjz5s168cUXe3xOl8uldevWdXspD/0H69T/sUYPBtbpwcA69X/RWiPr5zwBAAAMZHy3HQAAgAXCEwAAgAXCEwAAgAXCEwAAgIV+E55KSkqUnp6uxMREeb1eVVVVfeP4o0ePyuv1KjExUWPHjtXWrVtj1OnAZbNG+/fv1/Tp0/Xwww8rKSlJWVlZ+uSTT2LY7cBl+7PU5cSJE3I6nXr66aej2yAk2a9TR0eHioqKlJaWJpfLpccee0zl5eUx6nZgsl2jnTt3auLEiRoyZIiSk5O1ePFitba2xqjbgenYsWOaM2eOUlJS5HA4dPDgwW+d0yv5wfQDf/jDH8ygQYPMRx99ZGpra83y5cvN0KFDzaVLl7odf+HCBTNkyBCzfPlyU1tbaz766CMzaNAgs3fv3hh3PnDYrtHy5cvNW2+9Zf72t7+Zc+fOmcLCQjNo0CDzj3/8I8adDyy269Tl2rVrZuzYscbn85mJEyfGptkBLJJ1euGFF8yUKVOM3+83Fy9eNH/961/NiRMnYtj1wGK7RlVVVSYuLs68++675sKFC6aqqso8+eSTZu7cuTHufGCprKw0RUVFZt++fUaSOXDgwDeO76380C/C0+TJk01+fn5I7fHHHzdr1qzpdvxvfvMb8/jjj4fUXn75ZfPMM89ErceBznaNuvPEE0+Y9evX93Zr+H8iXafc3Fzz29/+1qxbt47wFAO26/THP/7RuN1u09raGov2YOzX6He/+50ZO3ZsSG3z5s1mzJgxUesRoXoSnnorP/T5Zbtbt26purpaPp8vpO7z+XTy5Mlu55w6dSps/IwZM3T69Gndvn07ar0OVJGs0X+7e/eu2tvbe/3LGfEfka7T9u3bdf78ea1bty7aLUKRrdOhQ4eUmZmpt99+W6NHj9b48eO1atUqff3117FoecCJZI2ys7N1+fJlVVZWyhijK1euaO/evZo9e3YsWkYP9VZ+sH7CeG9raWlRZ2dn2BcLezyesC8U7tLU1NTt+Dt37qilpUXJyclR63cgimSN/ts777yjGzduaN68edFoEYpsnb744gutWbNGVVVVcjr7/I+DASGSdbpw4YKOHz+uxMREHThwQC0tLXrllVd09epV7nuKgkjWKDs7Wzt37lRubq7+9a9/6c6dO3rhhRf03nvvxaJl9FBv5Yc+33nq4nA4Ql4bY8Jq3za+uzp6j+0addm9e7fefPNNVVRUaOTIkdFqD//W03Xq7OzU/PnztX79eo0fPz5W7eHfbH6e7t69K4fDoZ07d2ry5MmaNWuWNm3apB07drD7FEU2a1RbW6tly5Zp7dq1qq6u1uHDh3Xx4sXg976i/+iN/NDn/6s5YsQIxcfHh6X55ubmsHTYZdSoUd2OdzqdGj58eNR6HagiWaMuFRUVWrJkifbs2aNp06ZFs80Bz3ad2tvbdfr0adXU1Oi1116TdO+XtDFGTqdTR44c0fPPPx+T3geSSH6ekpOTNXr0aLnd7mAtIyNDxhhdvnxZ48aNi2rPA00ka1RcXKypU6dq9erVkqSnnnpKQ4cOVU5OjjZu3MgVkX6it/JDn+88JSQkyOv1yu/3h9T9fr+ys7O7nZOVlRU2/siRI8rMzNSgQYOi1utAFckaSfd2nBYtWqRdu3Zx3T8GbNcpKSlJn332mc6cORM88vPz9cMf/lBnzpzRlClTYtX6gBLJz9PUqVP11Vdf6fr168HauXPnFBcXpzFjxkS134EokjW6efOm4uJCf6XGx8dL+s/OBvper+UHq9vLo6Trr4SWlZWZ2tpas2LFCjN06FDzz3/+0xhjzJo1a8yCBQuC47v+quHKlStNbW2tKSsr41EFUWa7Rrt27TJOp9Ns2bLFNDY2Bo9r16711UcYEGzX6b/xt+1iw3ad2tvbzZgxY8zPf/5z8/nnn5ujR4+acePGmaVLl/bVR/ifZ7tG27dvN06n05SUlJjz58+b48ePm8zMTDN58uS++ggDQnt7u6mpqTE1NTVGktm0aZOpqakJPlIiWvmhX4QnY4zZsmWLSUtLMwkJCWbSpEnm6NGjwX+2cOFC8+yzz4aM//Of/2x+9KMfmYSEBPPoo4+a0tLSGHc88Nis0bPPPmskhR0LFy6MfeMDjO3P0v9HeIod23Wqq6sz06ZNM4MHDzZjxowxBQUF5ubNmzHuemCxXaPNmzebJ554wgwePNgkJyebX/ziF+by5csx7npg+dOf/vSNv2uilR8cxrCfCAAA0FN9fs8TAADAg4TwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYIHwBAAAYOH/AMwlDNb8bCyOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain predictions for the test set\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Identify misclassified indices\n",
    "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "# Map class indices to class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Display some misclassified images\n",
    "num_misclassified_to_display = 10  # Number of misclassified images to display\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, index in enumerate(misclassified_indices[:num_misclassified_to_display]):\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img_path = test_generator.filepaths[index]\n",
    "    img = keras_image.load_img(img_path, target_size=(128, 128))\n",
    "    img = keras_image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0  # Rescale if needed\n",
    "    \n",
    "    plt.imshow(img[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    true_label = class_labels[true_labels[index]]\n",
    "    predicted_label = class_labels[predicted_labels[index]]\n",
    "    plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4f6f1-4de6-4c29-a656-1d72b996ac36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ac30d-4406-4b21-90bf-f2deacc203fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unsupervised Learning - Apply only after training is complete\n",
    "\n",
    "# # Extract features\n",
    "# feature_model = Model(inputs=model.input, outputs=model.layers[-2].output)  # Using the layer before the output\n",
    "# features = feature_model.predict(train_generator)  # Using the training set for feature extraction\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA(n_components=50)\n",
    "# reduced_features = pca.fit_transform(features)\n",
    "\n",
    "# # Apply K-Means Clustering\n",
    "# kmeans = KMeans(n_clusters=10)\n",
    "# clusters = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "# # Visualize clusters\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.title('K-Means Clustering of Plant and Ailment Features')\n",
    "# plt.xlabel('PCA Component 1')\n",
    "# plt.ylabel('PCA Component 2')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e009e00-c7d2-432a-9f6c-a5ac36b40276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

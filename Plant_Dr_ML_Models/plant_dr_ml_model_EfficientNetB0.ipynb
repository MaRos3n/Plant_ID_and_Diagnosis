{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e1d33e-970a-43fe-9e6c-b24bb3414e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df9349-1938-460b-a9a1-7970ceb0cef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 53709\n",
      "Validation set size: 11509\n",
      "Test set size: 11510\n",
      "Found 53709 validated image filenames belonging to 167 classes.\n",
      "Found 11509 validated image filenames belonging to 167 classes.\n",
      "Found 11510 validated image filenames belonging to 167 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1679/1679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m812s\u001b[0m 478ms/step - accuracy: 0.0680 - loss: 4.2192 - val_accuracy: 0.0698 - val_loss: 4.1718\n",
      "Epoch 2/5\n",
      "\u001b[1m1679/1679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 312ms/step - accuracy: 0.0687 - loss: 4.1712 - val_accuracy: 0.0698 - val_loss: 4.1569\n",
      "Epoch 3/5\n",
      "\u001b[1m1679/1679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 192ms/step - accuracy: 0.0695 - loss: 4.1549 - val_accuracy: 0.0698 - val_loss: 4.1482\n",
      "Epoch 4/5\n",
      "\u001b[1m1679/1679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.0668 - loss: 4.1578"
     ]
    }
   ],
   "source": [
    "# Supervised Learning\n",
    "\n",
    "# Paths\n",
    "merged_dataset_path = '/Users/maggie/Desktop/project_3/Plant_ID_and_Diagnosis/Resources/plant_dr_master_dataset/master_dataset'\n",
    "combined_csv_path = '/Users/maggie/Desktop/project_3/Plant_ID_and_Diagnosis/Resources/plant_dr_master_dataset/master_dataset/combined_labels.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "\n",
    "# Print the distribution to verify the split\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(valid_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# ImageDataGenerator setup\n",
    "img_size = (128, 128)\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load training data using flow_from_dataframe\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',  # Column name in the CSV file that contains the image file names\n",
    "    y_col='label',  # Column name in the CSV file that contains the labels\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load validation data using flow_from_dataframe\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=valid_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load test data using flow_from_dataframe\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=merged_dataset_path,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # No need to shuffle for testing\n",
    ")\n",
    "\n",
    "# Load the base model (EfficientNetB0)\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x1 = GlobalAveragePooling2D()(base_model.output)\n",
    "x2 = Dense(1024, activation='relu')(x1)\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(train_generator.class_indices)  # Number of unique classes\n",
    "\n",
    "# Output layer\n",
    "x3 = Dense(num_classes, activation='softmax')(x2)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x3)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=valid_generator, epochs=5)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609b2e8-ab91-4683-928b-8a060b34a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(valid_generator)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ac30d-4406-4b21-90bf-f2deacc203fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Learning - Apply only after training is complete\n",
    "\n",
    "# Extract features\n",
    "feature_model = Model(inputs=model.input, outputs=model.layers[-2].output)  # Using the layer before the output\n",
    "features = feature_model.predict(train_generator)  # Using the training set for feature extraction\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=50)\n",
    "reduced_features = pca.fit_transform(features)\n",
    "\n",
    "# Apply K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "clusters = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=clusters, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('K-Means Clustering of Plant and Ailment Features')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e009e00-c7d2-432a-9f6c-a5ac36b40276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
